# Что пошло не так?

Во-первых, чтобы использовать LSTM/GRU - слои, данные должны быть нормализированны.
Допустим, мы используем StandardScaler [`(X - mu) / sigma`]. Числа для
`mu` и `sigma` получаются порядка `8.93e183`. Решается использованием `np.float132`
в `pad_sequences` и в остальных местах. Странно, но предсказание модели все равно
получается в `np.float32` (но это не суть).

После нормализации, лосс действительно
падает, но остается на одном и том же уровне на всех эпохах. Последующее
предсказание для обучающей выборки даёт одно и то же число, для любой
входящей последовательности.

# Как исправить?

seq2seq-обработка, которая заключается в работе над строками. Таким образом,
помимо различных функций генерации последовательности, RNN должна так же понять
что в строке содержится число, и над ним делается какая-то операция.
